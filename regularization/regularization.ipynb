{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "---\n",
    "Regularization helps to prevent model from overfitting by adding an extra penelization term at the end of the loss function.\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Where $m$ is the batch size. The shown regularization is called `L2 regularization`, which `L2` applies square to weights, `L1 regularization` applies absolute value, which has the form of $|W|$.\n",
    "\n",
    "The appended extra term would enlarge the loss if either there are two many weights or the weight becomes too large, and the adjustable factor $\\lambda$ emphasis on how much we want to penalize on the weights.\n",
    "\n",
    "_**1. Why penalizing weights would help to prevent overfitting?**_\n",
    "\n",
    "An intuitive understanding would be that in the process of minizing the new loss function, some of the weights would decrease close to zero so that the corresponding neurons would have very small effect to our results, as if we are trainig on a smaller neural network with fewer neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "In the forward process, we need only to change the loss function. let's review the cost function we've built in `deepNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import deepNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepNN([2, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7512649762748712\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[.3, .5, .7]])\n",
    "Y = np.array([[1, 1, 1]])\n",
    "\n",
    "loss = model.compute_cost(A, Y)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A, Y, parameters, reg=True, lambd=.2):\n",
    "    \"\"\"\n",
    "    With L2 regularization\n",
    "    parameters: dict with 'W1', 'b1', 'W2', ...\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    n_layer = len(parameters)//2\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    if reg:\n",
    "        p = 0\n",
    "        for i in range(1, n_layer+1):\n",
    "            p += np.sum(np.square(parameters['W'+str(i)]))\n",
    "        loss += (1/m)*(lambd/2)*p\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00224882, -0.00683036],\n",
       "        [-0.0155842 ,  0.00439355],\n",
       "        [ 0.0026745 ,  0.00287223],\n",
       "        [-0.00977243,  0.00515391]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.02002206,  0.00227708,  0.00470624,  0.00502016]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights_init()\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7512951351356093\n"
     ]
    }
   ],
   "source": [
    "loss = compute_loss(A, Y, model.params)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward\n",
    "---\n",
    "The backward propagation of `L2 reglularization` is actually straight forward, we only need to add the gradient of the L2 term.\n",
    "\n",
    "$$ \\underbrace{\\frac{\\partial{J}^{\\text{L2 Reg}}}{\\partial{W}}}_{\\text{new gradient}} = \\underbrace{ \\frac{\\partial{J}^{\\text{old}}}{\\partial{W}} }_{\\text{new gradient}} + \\frac{\\lambda}{m}|W|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y, lambd=0.2):\n",
    "    \"\"\"\n",
    "    params: weight [W, b]\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "        \n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_grad(A, Z))\n",
    "        \n",
    "        # with an extra gradient at the end, other terms would remain the same\n",
    "        dW = np.dot(dZ, A_prev.T)/m + (lambd/m)*W\n",
    "        \n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
