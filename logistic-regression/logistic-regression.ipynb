{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "---\n",
    "Say we are doing a classic perdiction task, where given a input vector with $n$ variables: \n",
    "\n",
    "$$ x = (x_1, x_2, ..., x_n)$$\n",
    "\n",
    "And to predict 1 response variable $y$ (may be the sales of next year, the house price, etc.), the simplest form is to use a linear regression to do the prediction with the formula:\n",
    "\n",
    "$$ y = W^{T}x + b$$\n",
    "\n",
    "Where $W$ is a column vector with $n$ dimension. Say now our question changed a bit, we hope to predict a probability, like what's the probability of raining tomorror? In this sense, this linear regression might be a little unfit here, as a linear expression can be unbounded and our probability is ranged in $[0, 1]$.\n",
    "\n",
    "---\n",
    "## Sigmoid Function\n",
    "---\n",
    "To bound our prediction in $[0, 1]$, the widely used technic is to apply a $sigmoid$ function:\n",
    "\n",
    "$$ z = \\sigma(x) = \\frac{1}{1+\\exp^{-x}}$$\n",
    "\n",
    "With `numpy` we can easily visualize the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11e61ea50>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10, 5])\n",
    "x = np.linspace(-10, 10, 2000)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "---\n",
    "The definition of loss function of logistic regression is defined as:\n",
    "\n",
    "$$ L(\\hat{y}, y) = - [ylog\\hat{y} + (1 - y)log(1 - \\hat{y})]$$\n",
    "\n",
    "Where $\\hat{y}$ is our prediction ranging from $[0, 1]$ and $y$ is the true value. When the actual value is $y = 1$, the equation becomes:\n",
    "\n",
    "$$L(\\hat{y}, y) = - ylog\\hat{y} $$\n",
    "\n",
    "the closer $\\hat{y}$ to 1, the smaller our loss is. And the same goes for $y = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula of One Input\n",
    "---\n",
    "Now to put everything together, for an given input $x = (x_1, x_2, ..., x_n)$ (with $n$ features), the forward path of logistic regression would be:\n",
    "\n",
    "$$ z = W^Tx + b$$\n",
    "\n",
    "$$ \\hat{y} = \\sigma{(z)}$$\n",
    "\n",
    "$$ L(\\hat{y}, y) = - [ylog\\hat{y} + (1 - y)log(1 - \\hat{y})]$$\n",
    "\n",
    "## Gradient Descent\n",
    "---\n",
    "Given this actual value $y$, we hope to minimize the loss $L$, and the technic we are going to apply here is gradient descent(the details has been illustrated in many articles), bascically what we need to do is to apply derivative to our variables and move the them slightly to down to the optimum.\n",
    "\n",
    "Here we have 2 variables, $W$ and $b$, and for this example, the update formula of them would be:\n",
    "\n",
    "$$W = W - \\frac{dL}{dW}$$\n",
    "\n",
    "$$b = b - \\frac{dL}{db}$$\n",
    "\n",
    "Where $W$ is a column vector with $n$ weights correpond to the $n$ dimension of $x^{(i)}$. In order to get the derivative of our targets, chain rules would be applied:\n",
    "\n",
    "$$\\frac{dL}{dW} = \\frac{dL}{d\\hat{y}} \\times \\frac{d\\hat{y}}{dz} \\times \\frac{dz}{dW} = (\\hat{y} - y)x$$\n",
    "\n",
    "$$\\frac{dL}{db} = \\frac{dL}{d\\hat{y}} \\times \\frac{d\\hat{y}}{dz} \\times \\frac{dz}{db} = (\\hat{y} - y)$$\n",
    "\n",
    "You can try out the deduction on your own, the only tricky part is the derivative of `sigmoid` function, for a good explanation you can refer to [here](https://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Training\n",
    "---\n",
    "The above gives the forward and backward updating process, which is well enough to implement a logistic regression if we were to feed in our training model *ONE AT A TIME*. However, in most training cases, we don't do that. Instead training samples are feed in batches, and the backward propogation is updated with average loss of the batch.\n",
    "\n",
    "Which means that for a model that feed with `m` samples at a time, the loss function would be:\n",
    "\n",
    "$$J(W, b) = \\frac{1}{m}\\sum_i{L(\\hat{y}^{(i)}, y^{(i)})}$$\n",
    "\n",
    "Where $i$ denotes the $ith$ training sample.\n",
    "\n",
    "## Forward Propogation of Batch Training\n",
    "---\n",
    "Now instead of using $x$, a single vector, as our input, we specify a matrix $X$ with size $n \\times m$, where as above, $n$ is the number of features and $m$ is number of training samples (basically, we line up $m$ training samples in a matrix). Now the formula becomes:\n",
    "\n",
    "$$ Z = W^TX + b \\tag1$$\n",
    "\n",
    "$$ \\hat{Y} = \\sigma{(Z)} \\tag2$$\n",
    "\n",
    "$$ L(\\hat{Y}, Y) = - [Ylog\\hat{Y} + (1 - Y)log(1 - \\hat{Y})] \\tag3$$\n",
    "\n",
    "$$J(W, b) = \\frac{1}{m}\\sum_i{L(\\hat{y}^{(i)}, y^{(i)})} \\tag4$$\n",
    "\n",
    "Note that here we use *UPPER LETTER* to denote our matrix and vectors (a caveat is that $b$ here is still a single value, the more formal way would be to represent $b$ as a vector, but in `python` the addition of a single value to a matrix would be automated broadcasted). \n",
    "\n",
    "Let's break down the size of the matrices one by one. \n",
    "\n",
    "In equation $(1)$, $W^T$ is a $1 \\times n$ matrix, $X$ is a $n \\times m$ matrix, so $Z$ is a $1 \\times m$ matrix\n",
    "\n",
    "In equetion $(2)$, applying directly the $sigmoid$ function to $Z$ would leave $\\hat{Y}$ still a $1 \\times m$ matrix\n",
    "\n",
    "In equetion $(3)$, the actual value matrix $Y$ is a $1 \\times m$ matrix, and all operations are element wise, so $L$ would still be a $1 \\times m$ matrix\n",
    "\n",
    "In the last equation, $J$ would be a single value denoting the loss of that batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation of Batch\n",
    "---\n",
    "Now give our new targe $J$, our derivative of variable $W$ and $b$ would be:\n",
    "\n",
    "$$ \\frac{dJ}{dW} = \\frac{dJ}{dL} \\times \\frac{dL}{dW} = \\frac{1}{m}X(\\hat{Y} - Y)^T$$\n",
    "\n",
    "$$ \\frac{dJ}{db} = \\frac{dJ}{dL} \\times \\frac{dL}{db} = \\frac{1}{m}\\sum_i{(\\hat{y}^{(i)} - y^{(i)})}$$\n",
    "\n",
    "Where $\\frac{dJ}{dW}$ is $1 \\times n$ vector and $\\frac{dJ}{db}$ is a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Classification Task\n",
    "---\n",
    "Our formula stuff ends here, let's implement our algorithm, before that some data needs to be generated to make a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_classification(n_samples=1000, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:700], X[700:]\n",
    "y_train, y_test = y[:700], y[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (700, 20)\n",
      "test shape (300, 20)\n"
     ]
    }
   ],
   "source": [
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "---\n",
    "Now everything is set, let's go for the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "---\n",
    "- sigmoid function that take in an array\n",
    "- weight function that initialize of $W$ and $b$ to zeros\n",
    "- accuracy function to measure our accuracy of binary prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def init_weights(dim):\n",
    "    W = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return W, b\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    y_pred, y_true = np.squeeze(y_pred), np.squeeze(y_true)\n",
    "    assert y_pred.shape == y_true.shape\n",
    "    y_pred = np.array([1 if i > 0.5 else 0 for i in y_pred])\n",
    "    return 1 - np.sum(np.abs(y_pred - y_true))/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1) 0\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "W, b = init_weights(10)\n",
    "\n",
    "print(W.shape, b)\n",
    "\n",
    "pred = np.array([0.2, 0.4, 0.7, 0.9])\n",
    "true = np.array([0, 1, 1, 0])\n",
    "acc = accuracy(pred, true)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "---\n",
    "Our `predict` function would be simply going through the forward process given trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    Y_hat = sigmoid(Z)\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "---\n",
    "Notice that for the `train` function, the input shape of $X$ needs to have shape of $n \\times m$, and $Y$ with shape of $1 \\times m$, where $m$ is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, learning_rate=0.01, iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    X: shape of n x m (n features and m samples)\n",
    "    y: shape 1 x m\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    m = X.shape[1]\n",
    "    W, b = init_weights(n)\n",
    "    assert W.shape == (n, 1)\n",
    "    assert Y.shape == (1, m)\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(iterations):\n",
    "        Z = np.dot(W.T, X) + b\n",
    "        Y_hat = sigmoid(Z)\n",
    "        assert Y_hat.shape == (1, m)\n",
    "\n",
    "        loss = -1/m * np.sum((Y*np.log(Y_hat) + (1-Y)*np.log(1 - Y_hat)))\n",
    "        losses.append(loss)\n",
    "        \n",
    "        dW = np.dot(X, (Y_hat - Y).T)/m\n",
    "        db = np.mean(Y_hat - Y)\n",
    "        \n",
    "        W -= dW\n",
    "        b -= db\n",
    "        \n",
    "        if verbose and i % 100 == 0:\n",
    "            print(f'round {i}: loss {loss}')\n",
    "            \n",
    "    return {'W': W, 'b': b, 'loss': losses}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input need to transpose in order to fit in our training requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (20, 700)\n",
      "Y shape (1, 700)\n"
     ]
    }
   ],
   "source": [
    "X = X_train.T\n",
    "Y = y_train.reshape(1, -1)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('Y shape', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1: loss 0.6931471805599454\n",
      "round 101: loss 0.2360053825603893\n",
      "round 201: loss 0.23599594434568905\n",
      "round 301: loss 0.23599593567130606\n",
      "round 401: loss 0.23599593566305357\n"
     ]
    }
   ],
   "source": [
    "model = train(X, Y, learning_rate=0.01, iterations=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model['W']\n",
    "b = model['b']\n",
    "\n",
    "y_pred = predict(X_test.T, W, b)\n",
    "y_true = y_test.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 93.67%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_pred, y_true)\n",
    "print('accuracy {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble in a Class\n",
    "---\n",
    "Now let's ensemble everything into a class to look more structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.W = 0\n",
    "        self.b = 0\n",
    "        self.losses = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(dim):\n",
    "        W = np.zeros((dim, 1))\n",
    "        b = 0\n",
    "        return W, b\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred, y_true):\n",
    "        y_pred, y_true = np.squeeze(y_pred), np.squeeze(y_true)\n",
    "        assert y_pred.shape == y_true.shape\n",
    "        y_pred = np.array([1 if i > 0.5 else 0 for i in y_pred])\n",
    "        return 1 - np.sum(np.abs(y_pred - y_true))/len(y_true)\n",
    "    \n",
    "    def train(self, X_train, y_train, learning_rate=0.01, iterations=1000, verbose=False):\n",
    "        \"\"\"\n",
    "        X: shape of n x m (n features and m samples)\n",
    "        y: shape 1 x m\n",
    "        \"\"\"\n",
    "        X = X_train.T\n",
    "        Y = y_train.reshape(1, -1)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        m = X.shape[1]\n",
    "        self.W, self.b = self.init_weights(n)\n",
    "        assert self.W.shape == (n, 1)\n",
    "        assert Y.shape == (1, m)\n",
    "\n",
    "        for i in range(iterations):\n",
    "            Z = np.dot(self.W.T, X) + self.b\n",
    "            Y_hat = self.sigmoid(Z)\n",
    "            assert Y_hat.shape == (1, m)\n",
    "\n",
    "            loss = -1/m * np.sum((Y*np.log(Y_hat) + (1-Y)*np.log(1 - Y_hat)))\n",
    "            self.losses.append(loss)\n",
    "\n",
    "            dW = np.dot(X, (Y_hat - Y).T)/m\n",
    "            db = np.mean(Y_hat - Y)\n",
    "\n",
    "            self.W -= dW\n",
    "            self.b -= db\n",
    "\n",
    "            if verbose and i % 100 == 0:\n",
    "                print(f'round {i}: loss {loss}')\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X = X_test.T\n",
    "        Z = np.dot(self.W.T, X) + self.b\n",
    "        Y_hat = self.sigmoid(Z)\n",
    "        return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0: loss 0.6931471805599454\n",
      "round 100: loss 0.13913472257785545\n",
      "round 200: loss 0.13580982295749638\n",
      "round 300: loss 0.13516585548864551\n",
      "round 400: loss 0.13499557852028424\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.train(X_train, y_train, learning_rate=0.01, iterations=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 94.00%\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_true = y_test.reshape(1, -1)\n",
    "\n",
    "acc = clf.accuracy(y_pred, y_true)\n",
    "print('accuracy {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
