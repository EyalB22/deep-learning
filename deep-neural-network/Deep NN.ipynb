{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Multi-Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "---\n",
    "Firstly, weights need to be initialized for different layers. Note that in general, the input is not considered as a layer, but output is.\n",
    "\n",
    "For `lth` layer, \n",
    "- weight $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$\n",
    "- bias $b^{[l]}$ has shape $(n^{[l]}, 1)$\n",
    "\n",
    "where $n^{[0]}$ equals the number input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layers_dim):\n",
    "    params = {}\n",
    "    \n",
    "    n = len(layers_dim)\n",
    "    for i in range(1, n):\n",
    "        params['W' + str(i)] = np.random.randn(layers_dim[i], layers_dim[i-1])*0.01\n",
    "        params['b' + str(i)] = np.zeros((layers_dim[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01031417,  0.0061496 ],\n",
       "        [-0.00091493,  0.00129659],\n",
       "        [-0.00982318, -0.00387674],\n",
       "        [ 0.01422292, -0.00017641],\n",
       "        [-0.00823383,  0.00041559]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00201449,  0.01214905, -0.00376643,  0.007895  , -0.01722043]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = weights_init([2, 5, 1])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "## Equations of Multi-layer\n",
    "---\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) $$\n",
    "\n",
    "Where $l$ is the `lth` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23147522 0.11920292 0.78583498] [0.  0.  1.3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1.2, -2.0, 1.3])\n",
    "\n",
    "sx = sigmoid(x)\n",
    "rx = relu(x)\n",
    "\n",
    "print(sx, rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    # intermediate layer use relu as activation\n",
    "    # last layer use sigmoid\n",
    "    n_layers = int(len(params)/2)\n",
    "    A = X\n",
    "    cache = {}\n",
    "    for i in range(1, n_layers):\n",
    "        W, b = params['W'+str(i)], params['b'+str(i)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "    \n",
    "    # last layer\n",
    "    W, b = params['W'+str(i+1)], params['b'+str(i+1)]\n",
    "    Z = np.dot(W, A) + b\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(i+1)] = Z\n",
    "    cache['A'+str(i+1)] = A\n",
    "    \n",
    "    return cache, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1., 1.]).reshape(2, 1)\n",
    "cache, A = forward(X, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[ 0.64609876],\n",
       "        [ 5.24933522],\n",
       "        [ 0.8979916 ],\n",
       "        [ 0.84440957],\n",
       "        [-1.9353641 ]]),\n",
       " 'A1': array([[0.64609876],\n",
       "        [5.24933522],\n",
       "        [0.8979916 ],\n",
       "        [0.84440957],\n",
       "        [0.        ]]),\n",
       " 'Z2': array([[7.31571729]]),\n",
       " 'A2': array([[0.99933544]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99933544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "---\n",
    "Still we consider this a binary classification, the cost of a batch would be:\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n",
    "\n",
    "Where $a$ is the predicted value, and $y$ is the actual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23101772979827936\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0.9, 0.3]])\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "loss = compute_cost(A, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "---\n",
    "<img src='images/backprop_kiank.png' style=\"width:800px;height:250px;\">\n",
    "<caption><center> **[source]**: https://github.com/enggen/Deep-Learning-Coursera </center></caption>\n",
    "\n",
    "The backward gradient can be calculated in recurrent fashion:\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) $$\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implementation of derivative of `sigmoid` and `relu` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(A, Z):\n",
    "    grad = np.multiply(A, 1-A)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def relu_grad(A, Z):\n",
    "    grad = np.zeros(Z.shape)\n",
    "    grad[Z>0] = 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1187803   0.73691718]\n",
      " [-0.27488196  2.50226182]\n",
      " [-1.85220012  1.02311522]] \n",
      "\n",
      "[[0.52966021 0.67632136]\n",
      " [0.43170897 0.92430023]\n",
      " [0.13561478 0.73557896]] \n",
      "\n",
      "[[0.24912027 0.21891078]\n",
      " [0.24533634 0.06996931]\n",
      " [0.11722341 0.19450255]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = sigmoid(z)\n",
    "g = sigmoid_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20147873 -0.80496658]\n",
      " [ 0.67536582  0.47002894]\n",
      " [ 0.30991813  1.32244906]] \n",
      "\n",
      "[[0.20147873 0.        ]\n",
      " [0.67536582 0.47002894]\n",
      " [0.30991813 1.32244906]] \n",
      "\n",
      "[[1. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = relu(z)\n",
    "g = relu_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the equations above, we have our implementation of backward propagation. Note that except the last layer where `sigmoid` function is used, the rest we all apply `relu` derivative to get the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    params: weight [W, b]\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "        \n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_grad(A, Z))\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW2': array([[-0.00042937, -0.00348851, -0.00059677, -0.00056116,  0.        ]]),\n",
       " 'db2': array([[-0.00066456]]),\n",
       " 'dW1': array([[-0.00028741, -0.00028741],\n",
       "        [-0.00088977, -0.00088977],\n",
       "        [ 0.00036544,  0.00036544],\n",
       "        [-0.00027917, -0.00027917],\n",
       "        [ 0.        ,  0.        ]]),\n",
       " 'db1': array([[-0.00028741],\n",
       "        [-0.00088977],\n",
       "        [ 0.00036544],\n",
       "        [-0.00027917],\n",
       "        [ 0.        ]])}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = backward(p, cache, np.array([[1], [1]]), np.array([[1]]))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the gradients, we have our weights updated as following:\n",
    "    \n",
    "$$ W^{[l]} -= dW^{[l]} $$\n",
    "$$ b^{[l]} -= db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, lr):\n",
    "    n_layers = int(len(params)/2)\n",
    "    for i in range(1, n_layers+1):\n",
    "        dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "        params['W'+str(i)] -= lr*dW\n",
    "        params['b'+str(i)] -= lr*db\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.1689771 ,  1.55166416],\n",
       "        [ 1.41478729,  3.34329237],\n",
       "        [ 0.85713757,  0.38010754],\n",
       "        [ 0.86450387, -0.14705302],\n",
       "        [-1.13190191, -1.41989977]]),\n",
       " 'b1': array([[ 0.3637177 ],\n",
       "        [ 0.80178421],\n",
       "        [-0.46679347],\n",
       "        [ 0.22439026],\n",
       "        [ 0.61643758]]),\n",
       " 'W2': array([[ 0.48185769,  1.74005544, -0.48127318,  0.48461999,  0.22424862]]),\n",
       " 'b2': array([[0.224251]])}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = optimize(p, g, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply on Dataset\n",
    "---\n",
    "Let's get our model apply on actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    \n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.round(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, layers: list, batch_size=200, n_iter=100, lr=0.1):\n",
    "    # prepare batch training\n",
    "    batches = generate_batch(X_train, batch_size)\n",
    "    # init weights\n",
    "    parameters = weights_init(layers)\n",
    "    for i in range(n_iter):\n",
    "        for batch in batches:\n",
    "            X = X_train[batch, :].T\n",
    "            Y = y_train[batch].reshape(1, -1)\n",
    "            cache, A = forward(X, parameters)\n",
    "            grads = backward(parameters, cache, X, Y)\n",
    "            parameters = optimize(parameters, grads, lr)\n",
    "            \n",
    "        if i%10 == 0:\n",
    "            loss = compute_cost(A, Y)\n",
    "            print(f'iteration {i}: loss {loss}')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 3-layer neural network, with input of 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6930569700112502\n",
      "iteration 10: loss 0.6929641356775633\n",
      "iteration 20: loss 0.6889342990091845\n",
      "iteration 30: loss 0.17372719641264908\n",
      "iteration 40: loss 0.07783859115402011\n",
      "iteration 50: loss 0.030366165914582934\n"
     ]
    }
   ],
   "source": [
    "trained_param = train(X_train, y_train, layers=[200, 20, 10, 1], batch_size=200, n_iter=60, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, pred = forward(X_test.T, trained_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 93.4%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
