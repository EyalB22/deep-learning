{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Multi-Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "---\n",
    "Firstly, weights need to be initialized for different layers. Note that in general, the input is not considered as a layer, but output is.\n",
    "\n",
    "For `lth` layer, \n",
    "- weight $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$\n",
    "- bias $b^{[l]}$ has shape $(n^{[l]}, 1)$\n",
    "\n",
    "where $n^{[0]}$ equals the number input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layers_dim):\n",
    "    params = {}\n",
    "    \n",
    "    n = len(layers_dim)\n",
    "    for i in range(1, n):\n",
    "        params['W' + str(i)] = np.random.randn(layers_dim[i], layers_dim[i-1])\n",
    "        params['b' + str(i)] = np.random.randn(layers_dim[i], 1)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[-1.20260404,  1.51803722],\n",
       "        [ 1.31068456,  3.23918964],\n",
       "        [ 0.89989452,  0.42286449],\n",
       "        [ 0.83184057, -0.17971632],\n",
       "        [-1.13190191, -1.41989977]]),\n",
       " 'b1': array([[ 0.33066558],\n",
       "        [ 0.69946101],\n",
       "        [-0.42476741],\n",
       "        [ 0.19228531],\n",
       "        [ 0.61643758]]),\n",
       " 'W2': array([[ 0.43247981,  1.33887682, -0.54990187,  0.42008628,  0.22424862]]),\n",
       " 'b2': array([[0.14716179]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = weights_init([2, 5, 1])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "## Equations of Multi-layer\n",
    "---\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) $$\n",
    "\n",
    "Where $l$ is the `lth` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23147522 0.11920292 0.78583498] [0.  0.  1.3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1.2, -2.0, 1.3])\n",
    "\n",
    "sx = sigmoid(x)\n",
    "rx = relu(x)\n",
    "\n",
    "print(sx, rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    # intermediate layer use relu as activation\n",
    "    # last layer use sigmoid\n",
    "    n_layers = int(len(params)/2)\n",
    "    A = X\n",
    "    cache = {}\n",
    "    for i in range(1, n_layers):\n",
    "        W, b = params['W'+str(i)], params['b'+str(i)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "    \n",
    "    # last layer\n",
    "    W, b = params['W'+str(i+1)], params['b'+str(i+1)]\n",
    "    Z = np.dot(W, A) + b\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(i+1)] = Z\n",
    "    cache['A'+str(i+1)] = A\n",
    "    \n",
    "    return cache, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1., 1.]).reshape(2, 1)\n",
    "cache, A = forward(X, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[ 0.64609876],\n",
       "        [ 5.24933522],\n",
       "        [ 0.8979916 ],\n",
       "        [ 0.84440957],\n",
       "        [-1.9353641 ]]),\n",
       " 'A1': array([[0.64609876],\n",
       "        [5.24933522],\n",
       "        [0.8979916 ],\n",
       "        [0.84440957],\n",
       "        [0.        ]]),\n",
       " 'Z2': array([[7.31571729]]),\n",
       " 'A2': array([[0.99933544]])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99933544]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "---\n",
    "Still we consider this a binary classification, the cost of a batch would be:\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n",
    "\n",
    "Where $a$ is the predicted value, and $y$ is the actual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23101772979827936\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0.9, 0.3]])\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "loss = compute_cost(A, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "---\n",
    "<img src='images/backprop_kiank.png' style=\"width:800px;height:250px;\">\n",
    "<caption><center> **[source]**: https://github.com/enggen/Deep-Learning-Coursera </center></caption>\n",
    "\n",
    "The backward gradient can be calculated in recurrent fashion:\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) $$\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
