{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Multi-Layer Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights Initialization\n",
    "---\n",
    "Firstly, weights need to be initialized for different layers. Note that in general, the input is not considered as a layer, but output is.\n",
    "\n",
    "For `lth` layer, \n",
    "- weight $W^{[l]}$ has shape $(n^{[l]}, n^{[l-1]})$\n",
    "- bias $b^{[l]}$ has shape $(n^{[l]}, 1)$\n",
    "\n",
    "where $n^{[0]}$ equals the number input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(layers_dim):\n",
    "    params = {}\n",
    "    \n",
    "    n = len(layers_dim)\n",
    "    for i in range(1, n):\n",
    "        params['W' + str(i)] = np.random.randn(layers_dim[i], layers_dim[i-1])*0.01\n",
    "        params['b' + str(i)] = np.zeros((layers_dim[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00578096, -0.0033889 ],\n",
       "        [-0.00802873, -0.00708873],\n",
       "        [-0.01977247,  0.00021493],\n",
       "        [ 0.00947858,  0.00554905],\n",
       "        [-0.01050366,  0.0114642 ]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00446584,  0.0011277 ,  0.00521519, -0.00092573,  0.00270877]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = weights_init([2, 5, 1])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "## Equations of Multi-layer\n",
    "---\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]}) $$\n",
    "\n",
    "Where $l$ is the `lth` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23147522 0.11920292 0.78583498] [0.  0.  1.3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([-1.2, -2.0, 1.3])\n",
    "\n",
    "sx = sigmoid(x)\n",
    "rx = relu(x)\n",
    "\n",
    "print(sx, rx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    # intermediate layer use relu as activation\n",
    "    # last layer use sigmoid\n",
    "    n_layers = int(len(params)/2)\n",
    "    A = X\n",
    "    cache = {}\n",
    "    for i in range(1, n_layers):\n",
    "        W, b = params['W'+str(i)], params['b'+str(i)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "    \n",
    "    # last layer\n",
    "    W, b = params['W'+str(i+1)], params['b'+str(i+1)]\n",
    "    Z = np.dot(W, A) + b\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(i+1)] = Z\n",
    "    cache['A'+str(i+1)] = A\n",
    "    \n",
    "    return cache, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([1., 1.]).reshape(2, 1)\n",
    "cache, A = forward(X, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[ 0.00239207],\n",
       "        [-0.01511746],\n",
       "        [-0.01955754],\n",
       "        [ 0.01502763],\n",
       "        [ 0.00096054]]),\n",
       " 'A1': array([[0.00239207],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.01502763],\n",
       "        [0.00096054]]),\n",
       " 'Z2': array([[-6.27109322e-07]]),\n",
       " 'A2': array([[0.49999984]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49999984]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "---\n",
    "Still we consider this a binary classification, the cost of a batch would be:\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â $$\n",
    "\n",
    "Where $a$ is the predicted value, and $y$ is the actual one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    \"\"\"\n",
    "    For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23101772979827936\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0.9, 0.3]])\n",
    "Y = np.array([[1, 0]])\n",
    "\n",
    "loss = compute_cost(A, Y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation\n",
    "---\n",
    "<img src='images/backprop_kiank.png' style=\"width:800px;height:250px;\">\n",
    "<caption><center> **[source]**: https://github.com/enggen/Deep-Learning-Coursera </center></caption>\n",
    "\n",
    "The backward gradient can be calculated in recurrent fashion:\n",
    "\n",
    "$$ dZ^{[l]} = dA^{[l]} * g^{[l]'}(Z^{[l]}) $$\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, implementation of derivative of `sigmoid` and `relu` is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(A, Z):\n",
    "    grad = np.multiply(A, 1-A)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def relu_grad(A, Z):\n",
    "    grad = np.zeros(Z.shape)\n",
    "    grad[Z>0] = 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.90176825 -0.719363  ]\n",
      " [-2.60503925  1.01777905]\n",
      " [ 0.62077097  0.10424591]] \n",
      "\n",
      "[[0.28868726 0.32753327]\n",
      " [0.06881481 0.73453976]\n",
      " [0.65039387 0.5260379 ]] \n",
      "\n",
      "[[0.20534692 0.22025523]\n",
      " [0.06407933 0.1949911 ]\n",
      " [0.22738168 0.24932203]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = sigmoid(z)\n",
    "g = sigmoid_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.08444414 -1.35978537]\n",
      " [ 0.58341162 -0.52831323]\n",
      " [ 1.48930994  1.84182838]] \n",
      "\n",
      "[[0.08444414 0.        ]\n",
      " [0.58341162 0.        ]\n",
      " [1.48930994 1.84182838]] \n",
      "\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "z = np.random.randn(3, 2)\n",
    "a = relu(z)\n",
    "g = relu_grad(a, z)\n",
    "\n",
    "print(z, '\\n')\n",
    "print(a, '\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the equations above, we have our implementation of backward propagation. Note that except the last layer where `sigmoid` function is used, the rest we all apply `relu` derivative to get the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    params: weight [W, b]\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "        \n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_grad(A, Z))\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dW2': array([[-0.00119603,  0.        ,  0.        , -0.00751382, -0.00048027]]),\n",
       " 'db2': array([[-0.50000016]]),\n",
       " 'dW1': array([[-0.00223292, -0.00223292],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ],\n",
       "        [ 0.00046287,  0.00046287],\n",
       "        [-0.00135439, -0.00135439]]),\n",
       " 'db1': array([[-0.00223292],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [ 0.00046287],\n",
       "        [-0.00135439]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = backward(p, cache, np.array([[1], [1]]), np.array([[1]]))\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given the gradients, we have our weights updated as following:\n",
    "    \n",
    "$$ W^{[l]} -= dW^{[l]} $$\n",
    "$$ b^{[l]} -= db^{[l]} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(params, grads, lr):\n",
    "    n_layers = int(len(params)/2)\n",
    "    for i in range(1, n_layers+1):\n",
    "        dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "        params['W'+str(i)] -= lr*dW\n",
    "        params['b'+str(i)] -= lr*db\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00801388, -0.00115598],\n",
       "        [-0.00802873, -0.00708873],\n",
       "        [-0.01977247,  0.00021493],\n",
       "        [ 0.00901571,  0.00508619],\n",
       "        [-0.00914928,  0.01281859]]),\n",
       " 'b1': array([[ 0.00223292],\n",
       "        [ 0.        ],\n",
       "        [ 0.        ],\n",
       "        [-0.00046287],\n",
       "        [ 0.00135439]]),\n",
       " 'W2': array([[0.00566187, 0.0011277 , 0.00521519, 0.00658809, 0.00318904]]),\n",
       " 'b2': array([[0.50000016]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = optimize(p, g, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply on Dataset\n",
    "---\n",
    "Let's have our model apply on created dataset with 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X, batch_size):\n",
    "    n = X.shape[0]\n",
    "    batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "    return batches\n",
    "\n",
    "\n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    \n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.round(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, layers: list, batch_size=200, n_iter=100, lr=0.1):\n",
    "    # prepare batch training\n",
    "    batches = generate_batch(X_train, batch_size)\n",
    "    # init weights\n",
    "    parameters = weights_init(layers)\n",
    "    for i in range(n_iter):\n",
    "        for batch in batches:\n",
    "            X = X_train[batch, :].T\n",
    "            Y = y_train[batch].reshape(1, -1)\n",
    "            cache, A = forward(X, parameters)\n",
    "            grads = backward(parameters, cache, X, Y)\n",
    "            parameters = optimize(parameters, grads, lr)\n",
    "            \n",
    "        if i%10 == 0:\n",
    "            loss = compute_cost(A, Y)\n",
    "            print(f'iteration {i}: loss {loss}')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a 3-layer neural network, with input of 200 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6930634457206598\n",
      "iteration 10: loss 0.693022813093077\n",
      "iteration 20: loss 0.6930210137413801\n",
      "iteration 30: loss 0.692971329136883\n",
      "iteration 40: loss 0.6804278680112462\n",
      "iteration 50: loss 0.17899414276926578\n"
     ]
    }
   ],
   "source": [
    "trained_param = train(X_train, y_train, layers=[200, 20, 10, 1], batch_size=200, n_iter=60, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache, pred = forward(X_test.T, trained_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 94.05%\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
