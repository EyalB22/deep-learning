{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "---\n",
    "- Zero Paddings\n",
    "- Convolutional Layer\n",
    "- Pooling\n",
    "- Combination: Conv + Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero Padding pads 0s at the edge of an image, benefits include:\n",
    "- It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer. \n",
    "\n",
    "- It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "\n",
    "<caption><center>from Deep Learning Specilization Course</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an input of batched images with shape:\n",
    "\n",
    "$$(m, n_W, n_H, n_C)$$\n",
    "\n",
    "Where $m$ is the batch size, $n_W$ is the width of the image, $n_H$ is the height and $n_C$ is number of channels -- RGB would has 3 channels.\n",
    "\n",
    "After padded with size $p$, the size would become\n",
    "\n",
    "$$(m, n_W + 2p, n_H + 2p, n_C)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pads(X, pad):\n",
    "    \"\"\"\n",
    "    X has shape (m, n_W, n_H, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (3, 4, 4, 3)\n",
      "X_pad shape (3, 8, 8, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'paded')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.random.randn(3, 4, 4, 3)\n",
    "X_pad = zero_pads(X, 2)\n",
    "\n",
    "print('X shape', X.shape)\n",
    "print('X_pad shape', X_pad.shape)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(X[0, :, :, 1], cmap='gray')\n",
    "plt.title('origin')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(X_pad[0, :, :, 1], cmap='gray')\n",
    "plt.title('paded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Step of Convolutional Layer\n",
    "---\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "Consider a filter mapped to one piece of the image, with \n",
    "\n",
    "$$ \\text{filter size:} \\quad (f, f, n_C) $$\n",
    "$$ \\text{piece of image} \\quad (f, f, n_C) $$\n",
    "\n",
    "Where filter has the deepth of the piece of input image.\n",
    "\n",
    "Another way to look at this is you can think of the filter as the weights $W$, and for each piece of the image, it serve as an input $X$, so in the convolutional process, the formula equals:\n",
    "\n",
    "$$ Z = sum(W*X) + b $$\n",
    "$$ A = g(Z) $$\n",
    "\n",
    "Where $b$ is the bias and $g$ is the activation function. Doesn't it look very similar to the equations in the dense neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def one_step_conv(X, W, b):\n",
    "    \"\"\"\n",
    "    X is the input, and W is the filter, both have the size (f, f, n_C)\n",
    "    b is the bias for this specific filter (note that different filters don't share the same bias)\n",
    "    Here suppose that we all take sigmoid as the activation function\n",
    "    \"\"\"\n",
    "    assert X.shape == W.shape\n",
    "    Z = np.sum(np.multiply(W, X)) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003152880008032791\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(10, 10, 3)\n",
    "W = np.random.randn(10, 10, 3)\n",
    "b = 0\n",
    "\n",
    "A = one_step_conv(X, W, b)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "---\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "Now the input (here we use `A_prev` ) would be a batch of whole images with size \n",
    "\n",
    "$$ (m, n_{W_{prev}}, n_{H_{prev}}, n_{C_{prev}}) $$\n",
    "\n",
    "Filter with size\n",
    "\n",
    "$$ (n_{C}, f, f, n_{C_{prev}}) $$\n",
    "\n",
    "Where $n_{C}$ is the number of filters, which would become the depth of the output image.\n",
    "\n",
    "Bias with size\n",
    "\n",
    "$$ (n_{C}, 1) $$\n",
    "\n",
    "And parameters include:\n",
    "$$ \\text{padding of each image:} \\enspace pad $$\n",
    "\n",
    "$$ \\text{moving step:} \\enspace stride $$\n",
    "\n",
    "So the resulting output would have size:\n",
    "\n",
    "$$ (m, \\lfloor\\frac{n_{W_{prev}} + 2p - f}{stride}\\rfloor  + 1, \\lfloor\\frac{n_{H_{prev}} + 2p - f}{stride}\\rfloor  + 1, n_C)$$\n",
    "\n",
    "Now given a image from the input, we will need to slice it into pieces and multiply with the filter one by one. \n",
    "\n",
    "Consider a 2D image with size $(n_{W_{prev}}, n_{H_{prev}})$, and stride is $s$, filter size if $f$, then the top-left corner of the output image would have mapping:\n",
    "```python\n",
    "input[0:(0 + f), 0:(0 + f)] -> output[0, 0]\n",
    "```\n",
    "\n",
    "And\n",
    "```python\n",
    "input[s:(s + f), 0:(0 + f)] -> output[1, 0]\n",
    "```\n",
    "\n",
    "The pattern would be:\n",
    "```python\n",
    "input[i*s:(i*s + f), j*s:(j*s + f)] -> output[i, j]\n",
    "```\n",
    "\n",
    "We will make use of this pattern in our implementation of slice the origin image and map to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(A_prev, filters, bias, parameters):\n",
    "    \"\"\"\n",
    "    A_prev: the input of batched images with shape: (m, n_W_prev, n_H_prev, n_C_prev)\n",
    "    filters has shape: (n_C, f, f, n_C_prev)\n",
    "    \"\"\"\n",
    "    \n",
    "    m, n_W_prev, n_H_prev, n_C_prev = A_prev.shape\n",
    "    pad, stride = parameters['pad'], parameters['stride']\n",
    "    n_C, f, f, _ = filters.shape\n",
    "    \n",
    "    n_W = (n_W_prev + 2*pad - f) // stride + 1\n",
    "    n_H = (n_H_prev + 2*pad - f) // stride + 1\n",
    "    \n",
    "    output = np.zeros((m, n_W, n_H, n_C))\n",
    "    padded_A_prev = zero_pads(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):\n",
    "        # take out the image\n",
    "        padded_img = padded_A_prev[i]\n",
    "        for c in range(n_C):\n",
    "            # take out filters and bias for the channel\n",
    "            fil = filters[c]\n",
    "            b = bias[c]\n",
    "            for w in range(n_W):\n",
    "                for h in range(n_H):\n",
    "                    w_range = (stride*w, stride*w + f)\n",
    "                    h_range = (stride*h, stride*h + f)\n",
    "                    img_slice = padded_img[w_range[0]:w_range[1], h_range[0]:h_range[1], :]\n",
    "                    output[i, w, h, c] = one_step_conv(img_slice, fil, b)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 15, 15, 10)\n"
     ]
    }
   ],
   "source": [
    "A_prev = np.random.randn(4, 28, 28, 3)\n",
    "filters = np.random.randn(10, 3, 3, 3)  # filter size (3, 3, 3)\n",
    "bias = np.zeros(10)\n",
    "parameters = {'pad': 2, 'stride': 2}\n",
    "\n",
    "Z = conv(A_prev, filters, bias, parameters)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling\n",
    "---\n",
    "After convolutional layer, it typically follows a pooling layer. The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: \n",
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "<caption><center>from Deep Learning Specilization Course</center></caption>\n",
    "\n",
    "The process is pretty much the same as the convolutional layer, with a filter and a stride, at each step, we will take a slice of the whole image and compute one value -- either max or average -- from it.\n",
    "\n",
    "Given filter size $f$, stride $s$ and input size:\n",
    "$$ (m, n_{W_{prev}}, n_{H_{prev}}, n_{C_{prev}}) $$\n",
    "\n",
    "Output would have size:\n",
    "\n",
    "$$ (m, \\lfloor\\frac{n_{W_{prev}} - f}{stride}\\rfloor  + 1, \\lfloor\\frac{n_{H_{prev}} - f}{stride}\\rfloor  + 1, n_C)$$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(A_prev, parameters, mode='max'):\n",
    "    \"\"\"\n",
    "    A_prev: the input of batched images with shape: (m, n_W_prev, n_H_prev, n_C_prev)\n",
    "    \"\"\"\n",
    "    m, n_W_prev, n_H_prev, n_C_prev = A_prev.shape\n",
    "    f, stride = parameters['f'], parameters['stride']\n",
    "    \n",
    "    n_W = (n_W_prev - f)//stride + 1\n",
    "    n_H = (n_H_prev - f)//stride + 1\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    output = np.zeros((m, n_W, n_H, n_C))\n",
    "    for i in range(m):\n",
    "        img = A_prev[i]\n",
    "        for w in range(n_W):\n",
    "            for h in range(n_H):\n",
    "                for c in range(n_C):\n",
    "                    w_range = (stride*w, stride*w + f)\n",
    "                    h_range = (stride*h, stride*h + f)\n",
    "                    img_slice = img[w_range[0]:w_range[1], h_range[0]:h_range[1], c]\n",
    "                    if mode == 'max':\n",
    "                        output[i, w, h, c] = np.max(img_slice)\n",
    "                    elif mode == 'average':\n",
    "                        output[i, w, h, c] = np.mean(img_slice)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 14, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "A_prev = np.random.randn(4, 28, 28, 3)\n",
    "parameters = {'f': 2, 'stride': 2}\n",
    "\n",
    "A = pooling(A_prev, parameters, mode='max')\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
