{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "---\n",
    "As neural network goes deeper and deeper, thorectically, it would perform better at least on training set, but in practice, the performance does not go better with deeper network. One major reason is `gradient vanishing` problem, in the backpropagation of a very deep network, the gradients at earlier layers would go to zero quickly that cause the learning process to be unbearably slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "---\n",
    "ResNet makes it possible, thoretically, to build infinite deeper neural network without impair the model perfermance, at least not getting worse. The major advanced structure in `Resnet` is called `skip connection`.\n",
    "\n",
    "<img src=\"images/images/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left side is a plain network without `skip connection`, the right side is the one with a `skip connection`. \n",
    "\n",
    "`skip connection` allows activation values in earlier layers to be fast-forwarded to latter layers by addition. These has 2 advantages:\n",
    "\n",
    "- In the forward propagation, the latter layer would at least have the performance of earlier layers:\n",
    "\n",
    "$$ a^{[l + 2]} = g(z^{[l+2]} + a^{[l]})$$\n",
    "\n",
    "Where $a^{[l]}$ is fast-forwarded from earlier layer. In the iteration process, as weights go to zero, $z$ would decrease to zero, in the worst scenario, even when $z$ totally lost its effect, activation value $a^{[l+2]}$ would at least have the value of $a^{[l]}$.\n",
    "\n",
    "- In the backpropagation process, the gradient can be directly backpropagated to earlier layers so that there would be less multiplications in the backprogaction, thus accelerate the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Block\n",
    "---\n",
    "There are 2 sorts of `skip connections` in convolutional network, the first one is called `identity block`, shown as below:\n",
    "\n",
    "<img src=\"images/images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "\n",
    "In this structure the activation value of earlier layer `x` is directly copied and added to the later layer before send to the `ReLU` activation.\n",
    "\n",
    "Here're the individual steps.\n",
    "\n",
    "First component of main path: \n",
    "- The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. Use 0 as the seed for the random initialization. \n",
    "- The first BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2a'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Second component of main path:\n",
    "- The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is \"same\" and its name should be `conv_name_base + '2b'`. Use 0 as the seed for the random initialization. \n",
    "- The second BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2b'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Third component of main path:\n",
    "- The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2c'`. Use 0 as the seed for the random initialization. \n",
    "- The third BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component. \n",
    "\n",
    "Final step: \n",
    "- The shortcut and the input are added together.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, nfilters, stage, block):\n",
    "    \"\"\"\n",
    "    X has shape (m, n_H, n_W, n_C)\n",
    "    f filter size of second conv + bn + relu\n",
    "    \"\"\"\n",
    "    assert len(nfilters) == 3\n",
    "    F1, F2, F3 = nfilters\n",
    "    \n",
    "    # note: for identity block, the last layer of input X need to has the same dimension of last filter\n",
    "    # to enable Addition in the end\n",
    "    assert F3 == X.shape[3]\n",
    "    conv_base_name = 'conv_' + str(stage) + block\n",
    "    bn_base_name = 'bn_' + str(stage) + block\n",
    "    \n",
    "    X_shortcut = X\n",
    "    # first\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), \n",
    "               padding='valid', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # second\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), \n",
    "               padding='same', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # third\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), \n",
    "               padding='valid', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2a')(X)  # X shape: (m, n_H, n_W, F3)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (3, 4, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 4, 4, 6)\n",
    "\n",
    "X_output = identity_block(X, f=2, nfilters=[2, 4, 6], stage='1', block='a')\n",
    "\n",
    "print('output shape', X_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
